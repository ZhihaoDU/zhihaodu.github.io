
<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>About | [“Welcome to Zhihao Du（杜志浩）’s homepage.”]</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="About" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://zhihaodu.github.io/" />
<meta property="og:url" content="https://zhihaodu.github.io/" />
<meta property="og:site_name" content="[“Welcome to Zhihao Du（杜志浩）’s homepage.”]" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="About" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","headline":"About","name":"[“Welcome to Zhihao Du（杜志浩）’s homepage.”]","url":"https://zhihaodu.github.io/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=b3e4a4daa47f83a7474c6268eaa95beeb8e0e9c4">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body">
      
      <h1><a href="https://zhihaodu.github.io/">Welcome to Zhihao Du（杜志浩）’s homepage.</a></h1>

      <h2 id="about">Highlights</h2>
      <ol>
        <li>CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training [Code][<a href="https://arxiv.org/abs/2505.17589">Paper</a>][<a href="https://funaudiollm.github.io/cosyvoice3/">Demos</a>]</li>
        <li>MinMo: A Multimodal Large Language Model for Seamless Voice Interaction [Code][<a href="https://arxiv.org/abs/2501.06282">Paper</a>][<a href="https://funaudiollm.github.io/minmo/">Demos</a>]</li>
        <li>CosyVoice 2: Scalable Streaming Speech Synthesis model has been open-sourced: [<a href="https://github.com/FunAudioLLM/CosyVoice">Code</a>][<a href="https://arxiv.org/abs/2412.10117">Paper</a>][<a href="https://funaudiollm.github.io/cosyvoice2/">Demos</a>]</li>
        <li>CosyVoice: Large-scale Multilingual Zero-shot Text-to-speech Synthesizer has been open-sourced: [<a href="https://github.com/FunAudioLLM/CosyVoice">Code</a>][<a href="https://arxiv.org/abs/2407.05407">Paper</a>][<a href="https://fun-audio-llm.github.io/">Demos</a>]</li>
        <li>FunAudioLLM has bee released at: [<a href="https://github.com/FunAudioLLM">Code</a>][<a href="https://arxiv.org/abs/2407.04051">Paper</a>]</li>
      </ol>

      <h2 id="about">About（关于）</h2>

<p>I'm a tech lead of Speech Team, Tongyi Lab, Alibaba group. My responsibility is building CosyVoice series, large speech generation models.
  I recieved the Ph.D. degree with the School of Computer Science and Technology at Harbin Institute of Technology under the supervision of Jiqing Han, in 2021. 
  I received the B.E. degree in software engineering from the College of Software of Inner Mongolia University under the supervision of Xueliang Zhang, in 2015. 
  My research interests include multi-modal large language models, generative models, speech processing and deep learning. 
  Last, but certainly not least, I'd like to thanks my wonderful wife for her understanding and supports.[<a href="dagv_demo/dzh.pdf">About Me</a>]</p>

<h2 id="publications">Publications（出版物）</h2>

<p>(Note: Most of my papers can be found on arxiv.)</p>

<h3 id="journal--papers">Journal  Papers（期刊论文）</h3>

<ol>
  <li><strong><u>Zhihao Du</u></strong>, Xueliang Zhang, Jiqing Han. A joint framework of denoising autoencoder and generative vocoder for monaural speech enhancement. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020. <a href="https://zhihaodu.github.io/dagv_demo/">View Demos</a></li>
</ol>

<h3 id="conference-papers">Conference Papers（会议论文）</h3>

<ol>
  <li>Qinglin Zhang, Luyao Cheng, Chong Deng, Qian Chen, Wen Wang, Siqi Zheng, Jiaqing Liu, Hai Yu, Chao-Hong Tan, <strong><u>Zhihao Du</u></strong>, ShiLiang Zhang, "OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation", ACL 2025</li>
  <li>Changfeng Gao, <strong><u>Zhihao Du</u></strong>, Shiliang Zhang,  Differentiable Reward Optimization for LLM based TTS system, INTERSPEECH 2025</li>
  <li>Xiang Lyu, Yuxuan Wang, Tianyu Zhao, Hao Wang, Huadai Liu, <strong><u>Zhihao Du</u></strong>, Build LLM-base Zero-Shot Streaming TTS System with CosyVoice. ICASSP 2025</li>
  <li>Guanrou Yang, Fan Yu, Ziyang Ma, <strong><u>Zhihao Du</u></strong>, Zhifu Gao, Shiliang Zhang, Xie Chen, Enhancing Low-Resource ASR through Versatile TTS: Bridging the Data Gap. ICASSP 2025</li>
  <li>Ziyang Ma, Guanrou Yang, Yifan Yang, Zhifu Gao, Jiaming Wang, <strong><u>Zhihao Du</u></strong>, Fan Yu, Qian Chen, Siqi Zheng, ShiLiang Zhang, Xie Chen, Speech Recognition Meets Large Language Model: Benchmarking, Models, and Exploration. AAAI 2025(Oral)</li>
  <li>Yue Gu, <strong><u>Zhihao Du</u></strong>, Shiliang Zhang, Jiqing Han, Yongjun He, Personality-aware Training based Speaker Adaptation for End-to-end Speech Recognition. INTERSPEECH 2024 <a href="https://www.isca-archive.org/interspeech_2024/gu24b_interspeech.pdf">Paper</a></li>
  <li><strong><u>Zhihao Du</u></strong>, Shiliang Zhang, Kai Hu, Siqi Zheng, Funcodec: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec. ICASSP 2024</li>
  <li>Mohan Shi, <strong><u>Zhihao Du</u></strong>, et al., A Comparative Study on Multichannel Speaker-Attributed Automatic Speech Recognition in Multi-Party Meetings. APSIPA 2023</li>
  <li>Yangze Li, Fan Yu, Yuhao Liang, Pengcheng Guo, Mohan Shi, <strong><u>Zhihao Du</u></strong>, Shiliang Zhang, Lei Xie, Sa-Paraformer: Non-Autoregressive End-To-End Speaker-Attributed ASR. ASRU 2023</li>
  <li>Yuhao Liang, Mohan Shi, Fan Yu, Yangze Li, Shiliang Zhang, <strong><u>Zhihao Du</u></strong>, Qian Chen, Lei Xie, Yanmin Qian, Jian Wu, Zhuo Chen, Kong Aik Lee, Zhijie Yan, Hui Bu, The Second Multi-Channel Multi-Party Meeting Transcription Challenge (M2MeT 2.0): A Benchmark for Speaker-Attributed ASR. ASRU 2023</li>
  <li>Yue Gu, <strong><u>Zhihao Du</u></strong>, Shiliang Zhang, Qian Chen, Jiqing Han, Personality-aware Training based Speaker Adaptation for End-to-end Speech Recognition. INTERSPEECH 2023 <a href="https://zhihaodu.github.io/gu2023pat.pdf">Paper</a></li>
  <li>Mohan Shi, <strong><u>Zhihao Du</u></strong>, Qian Chen, Fan Yu, Yangze Li, Shiliang Zhang, Jie Zhang, Lirong Dai, CASA-ASR: Context-Aware Speaker-Attributed ASR. INTERSPEECH 2023</li>
  <li>Zhifu Gao, Zerui Li, Jiaming Wang, Haoneng Luo, Xian Shi, Mengzhe Chen, Yabin Li, Lingyun Zuo, <strong><u>Zhihao Du</u></strong>, Zhangyu Xiao, Shiliang Zhang, FunASR: A Fundamental End-to-End Speech Recognition Toolkit. INTERSPEECH 2023</li>
  <li>Jiaming Wang*, <strong><u>Zhihao Du*</u></strong>, Shiliang Zhang. TOLD: A Novel Two-stage Overlap-aware Framework for Speaker Diarization. ICASSP 2023 (equal contribution)</li>
  <li><strong><u>Zhihao Du</u></strong>, Shiliang Zhang, Siqi Zheng, Zhijie Yan. Speaker Overlap-aware Neural Diarization for Multi-party Meeting Analysis. EMNLP 2022 (long paper)</li>
  <li>Yuxiao Lin, <strong><u>Zhihao Du</u></strong>, Shiliang Zhang, Fan Yu, Zhou Zhao, Fei Wu, Separate-to-Recognize: Joint Multi-target Speech Separation and Speech Recognition for Speaker-attributed ASR. ISCSLP 2022</li>
  <li>Fan Yu, Shiliang Zhang, Pengcheng Guo, Yuhao Liang, <strong><u>Zhihao Du</u></strong>, et.al. MFCCA: Multi-Frame Cross-Channel attention for multi-speaker ASR in Multi-party meeting scenario. SLT 2022</li>
  <li>Fan Yu, <strong><u>Zhihao Du</u></strong>, Shiliang Zhang, Yuxiao Lin, Lei Xie. A Comparative Study on Speaker-attributed Automatic Speech Recognition in Multi-party Meetings. ICASSP 2022</li>
  <li>Fan Yu, Shiliang Zhang, Pengcheng Guo, Yihui Fu, <strong><u>Zhihao Du</u></strong>, et.al. Summary on the ICASSP 2022 multi-channel multi-party meeting transcription grand challenge. ICASSP 2022</li>
  <li>Fan Yu, Shiliang Zhang, Yihui Fu, Lei Xie, Siqi Zheng, <strong><u>Zhihao Du</u></strong>, et.al. M2MeT: The ICASSP 2022 multi-channel multi-party meeting transcription challenge. ICASSP 2022</li>
  <li>Hongwei Song, Jiqing Han, Shiwen Deng, <strong><u>Zhihao Du</u></strong>. Capturing Temporal Dependencies Through Future Prediction for CNN-Based Audio Classifiers. ICASSP 2021</li>
  <li><strong><u>Zhihao Du</u></strong>, Ming Lei, Jiqing Han, Shiliang Zhang. Pan: Phoneme-aware network for monaural speech enhancement. ICASSP 2020.</li>
  <li><strong><u>Zhihao Du</u></strong>, Ming Lei, Jiqing Han, Shiliang Zhang. Self-Supervised Adversarial Multi-Task Learning for Vocoder-Based Monaural Speech Enhancement. INTERSPEECH 2020</li>
  <li><strong><u>Zhihao Du</u></strong>, Jiqing Han, Xueliang Zhang. Double Adversarial Network Based Monaural Speech Enhancement for Robust Speech Recognition. INTERSPEECH 2020, https://github.com/ZhihaoDU/du2020dan</li>
  <li>Yue Gu, <strong><u>Zhihao Du</u></strong>, Hui Zhang, Xueliang Zhang. An Efficient Joint Training Framework for Robust Small-Footprint Keyword Spotting. ICONIP 2020</li>
  <li>Hongwei Song, Jiqing Han, Shiwen Deng. <strong><u>Zhihao Du</u></strong>. Acoustic scene classification by implicitly identifying distinct sound events, INTERSPEECH 2019</li>
  <li><strong><u>Zhihao Du</u></strong>, Xueliang Zhang, Jiqing Han. Investigation of Monaural Front-End Processing for Robust Speech Recognition Without Retraining or Joint-Training. APSIPA 2019.</li>
</ol>
      
<h3 id="preprints">Preprints（预印本）</h3>

<ol>
  <li><strong><u>Zhihao Du</u></strong>, Shiliang Zhang, Siqi Zheng, Zhijie Yan. Speaker Embedding-aware Neural Diarization for Flexible Number of Speakers with Textual Information. https://arxiv.org/abs/2111.13694.</li>
</ol>

<h3 id="phd-thesis">PhD Thesis（博士论文）</h3>

<p>RESEARCH ON MONAURAL SPEECH ENHANCEMENT BASED ON PRIOR INFORMATION IN DIFFERENT SEMANTIC LEVELS（基于不同语义层级先验信息的
   单通道语音增强方法研究）.</p>

<h2 id="审稿">Reviewer（审稿)</h2>

<ol>
  <li>TASLP, IEEE Transactions on Audio, Speech and Language Processing</li>
  <li>ACM MM, ACM Multimedia</li>
  <li>IJCAI, International Joint Conference on Artificial Intelligence</li>
  <li>ICASSP, International Conference on Acoustics, Speech and Signal Processing</li>
  <li>INTERSPEECH, Conference of the International Speech Communication Association</li>
  <li>IAML, International Conference on Asian Language Processing</li>
  <li>ISCSLP, International Symposium on Chinese Spoken Language Processing</li>
</ol>

<h2 id="opensources">Open sources（开源代码）</h2>
<ol>
  <li>Widely-used speech features, https://github.com/ZhihaoDU/speech_feature_extractor, star 100+</li>
</ol>
  
<h2 id="荣誉">Honors（荣誉）</h2>
<ol>
  <li>哈尔滨工业大学优秀博士论文提名（2021）</li>
  <li>内蒙古自治区优秀毕业生（2015）</li>
  <li>MCM Meritorious Winner</li>
  <li>ACM/ICPC 二等奖</li>
</ol>
      
<h2 id="Organization">Organization（组织）</h2>
<ol>
  <li>IEEE Member</li>
  <li>SIGDAT Member</li>
</ol>

<h2 id="contact-me">Contact me（联系我）</h2>

<p>TEL: +86-15600609952</p>

<p>E-mails: duzhihao.china@gmail.com and neo.dzh@alibaba-inc.com</p>

      
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.0/anchor.min.js" integrity="sha256-lZaRhKri35AyJSypXXs4o6OPFTbTmUoltBbDCbdzegg=" crossorigin="anonymous"></script>
    <script>anchors.add();</script>
  </body>
</html>

